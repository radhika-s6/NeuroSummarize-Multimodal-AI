{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142c2fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook cwd: d:\\MSc_AI\\MSc_Project\\NeuroSummarize\\notebooks\n",
      "Added project root to sys.path: D:\\MSc_AI\\MSc_Project\\NeuroSummarize\n",
      "sys.path[0] -> D:\\MSc_AI\\MSc_Project\\NeuroSummarize\n",
      "src exists at: True\n",
      "Listing project root contents: ['.git', '.vscode', 'app.py', 'assets', 'config', 'data', 'nmslib', 'notebooks', 'outputs', 'README.md', 'reports', 'requirements.txt', 'run_pipeline.py', 'src', 'tools']\n"
     ]
    }
   ],
   "source": [
    "# --- Insert this as the FIRST code cell in the notebook ---\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Print current working dir (helpful check)\n",
    "print(\"Notebook cwd:\", Path.cwd())\n",
    "\n",
    "# Walk upwards until we find a folder that contains \"src\"\n",
    "proj_root = Path.cwd()\n",
    "while not (proj_root / \"src\").exists():\n",
    "    if proj_root.parent == proj_root:\n",
    "        raise RuntimeError(\"Could not find project root containing 'src' by walking up from cwd. \"\n",
    "                           \"Either run the notebook from the repo root, or create src/__init__.py, or set PYTHONPATH.\")\n",
    "    proj_root = proj_root.parent\n",
    "\n",
    "proj_root = proj_root.resolve()\n",
    "sys.path.insert(0, str(proj_root))\n",
    "print(\"Added project root to sys.path:\", proj_root)\n",
    "print(\"sys.path[0] ->\", sys.path[0])\n",
    "\n",
    "# Quick sanity checks\n",
    "print(\"src exists at:\", (proj_root / \"src\").exists())\n",
    "print(\"Listing project root contents:\", os.listdir(proj_root)[:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cc2bb0",
   "metadata": {},
   "source": [
    "\n",
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8afa6006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, pandas as pd\n",
    "from src.evaluate_models import load_gold_dataset, evaluate_one_model_on_dataset, plot_grouped_metrics, plot_radar_normalized, plot_metric_distributions\n",
    "from src.summarize import Summarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f45800f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def evaluate_one_model_on_dataset(model_id: str, items: List[Dict], summarizer,\n",
      "                                  max_samples: int = None, save_dir: Path = None) -> Dict:\n",
      "    \"\"\"\n",
      "    Evaluate a summarizer on a list of items, returning aggregated metrics.\n",
      "\n",
      "    Defensive / robustness improvements:\n",
      "    - Initialize `row` early so it cannot raise UnboundLocalError.\n",
      "    - Use safe lookups for etype score values.\n",
      "    - Capture and continue on per-item summarization exceptions.\n",
      "    - When saving CSV, include the union of all keys (so dynamic `entity_f_*` columns are preserved).\n",
      "    \"\"\"\n",
      "    results_rows = []\n",
      "    agg = {\n",
      "        'model': model_id,\n",
      "        'n': 0,\n",
      "        'rouge_l_clin': [], 'bleu_clin': [],\n",
      "        'rouge_l_lay': [], 'bleu_lay': [],\n",
      "        'entity_p': [], 'entity_r': [], 'entity_f': [],\n",
      "        'hall_rate': [], 'likert': []\n",
      "    }\n",
      "\n",
      "    for idx, item in enumerate(items):\n",
      "        if max_samples and idx >= max_samples:\n",
      "            break\n",
      "\n",
      "        # --- Basic case fields ---\n",
      "        doc_id = item.get(\"id\", f\"case_{idx}\")\n",
      "        text = item.get(\"report\", \"\")\n",
      "        gold_clin = item.get(\"clinical_summary_gold\", \"\")\n",
      "        gold_lay = item.get(\"lay_summary_gold\", \"\")\n",
      "        gold_entities = item.get(\"entities_gold\", [])\n",
      "\n",
      "        # Likert field (support two keys)\n",
      "        likert_score = item.get(\"clinical_utility_score\", item.get(\"clinical_utility_likert\", None))\n",
      "        if likert_score is not None:\n",
      "            try:\n",
      "                likert_score = float(likert_score)\n",
      "            except Exception:\n",
      "                likert_score = None\n",
      "\n",
      "        # Initialize row early to avoid UnboundLocalError in any code path\n",
      "        row: Dict[str, object] = {\"model\": model_id, \"id\": doc_id}\n",
      "\n",
      "        # --- Summarize (defensive) ---\n",
      "        try:\n",
      "            out = summarizer.summarize_both(text)\n",
      "        except Exception as e:\n",
      "            # If summarizer fails for this item, record the error and continue\n",
      "            row.update({\n",
      "                \"rouge_l_clin\": None, \"bleu_clin\": None,\n",
      "                \"rouge_l_lay\": None, \"bleu_lay\": None,\n",
      "                \"entity_p\": None, \"entity_r\": None, \"entity_f\": None,\n",
      "                \"hallucination_rate\": None,\n",
      "                \"clinical_utility_likert\": likert_score,\n",
      "                \"pred_entities\": json.dumps({\"error\": str(e)})\n",
      "            })\n",
      "            results_rows.append(row)\n",
      "            # Do not include this item's metrics in the aggregates (alternatively, you could include zeros)\n",
      "            continue\n",
      "\n",
      "        pred_clin = out.get(\"clinical_summary\", \"\")\n",
      "        pred_lay = out.get(\"lay_summary\", \"\")\n",
      "        # prefer structured pred_entities returned by Summarizer\n",
      "        pred_entities = out.get(\"pred_entities\")\n",
      "        if pred_entities is None:\n",
      "            pred_entities = parse_entities_from_summary(pred_clin, None)\n",
      "\n",
      "        # --- Compute metrics (assume functions exist in module scope) ---\n",
      "        r_clin = rouge_l_score(gold_clin, pred_clin)\n",
      "        b_clin = bleu_score(gold_clin, pred_clin)\n",
      "        r_lay = rouge_l_score(gold_lay, pred_lay)\n",
      "        b_lay = bleu_score(gold_lay, pred_lay)\n",
      "\n",
      "        p, r, f = entity_level_scores(gold_entities, pred_entities)\n",
      "        etype_scores = entity_level_scores_by_type(gold_entities, pred_entities)\n",
      "        hrate = hallucination_rate(gold_entities, pred_entities)\n",
      "\n",
      "        # --- Populate main row fields ---\n",
      "        # Use json.dumps for pred_entities to make CSV-friendly\n",
      "        try:\n",
      "            pred_entities_serialized = json.dumps(pred_entities)\n",
      "        except Exception:\n",
      "            # If pred_entities is not JSON-serializable directly, coerce to string\n",
      "            pred_entities_serialized = json.dumps({\"value\": str(pred_entities)})\n",
      "\n",
      "        row.update({\n",
      "            \"rouge_l_clin\": r_clin,\n",
      "            \"bleu_clin\": b_clin,\n",
      "            \"rouge_l_lay\": r_lay,\n",
      "            \"bleu_lay\": b_lay,\n",
      "            \"entity_p\": p,\n",
      "            \"entity_r\": r,\n",
      "            \"entity_f\": f,\n",
      "            \"hallucination_rate\": hrate,\n",
      "            \"clinical_utility_likert\": likert_score,\n",
      "            \"pred_entities\": pred_entities_serialized\n",
      "        })\n",
      "\n",
      "        # --- Safely add entity-level f-scores (dynamic keys) ---\n",
      "        if isinstance(etype_scores, dict):\n",
      "            for etype, vals in etype_scores.items():\n",
      "                key = f\"entity_f_{str(etype).lower()}\"\n",
      "                # vals might be None or missing 'f'\n",
      "                if isinstance(vals, dict):\n",
      "                    row[key] = vals.get(\"f\", None)\n",
      "                else:\n",
      "                    row[key] = None\n",
      "\n",
      "        results_rows.append(row)\n",
      "\n",
      "        # --- Update aggregates ---\n",
      "        agg['n'] += 1\n",
      "        if r_clin is not None:\n",
      "            agg['rouge_l_clin'].append(r_clin)\n",
      "        if b_clin is not None:\n",
      "            agg['bleu_clin'].append(b_clin)\n",
      "        if r_lay is not None:\n",
      "            agg['rouge_l_lay'].append(r_lay)\n",
      "        if b_lay is not None:\n",
      "            agg['bleu_lay'].append(b_lay)\n",
      "        if p is not None:\n",
      "            agg['entity_p'].append(p)\n",
      "        if r is not None:\n",
      "            agg['entity_r'].append(r)\n",
      "        if f is not None:\n",
      "            agg['entity_f'].append(f)\n",
      "        if hrate is not None:\n",
      "            agg['hall_rate'].append(hrate)\n",
      "        if likert_score is not None:\n",
      "            agg['likert'].append(likert_score)\n",
      "\n",
      "    # Save per-case CSV if requested (ensure union of all dynamic columns included)\n",
      "    if save_dir:\n",
      "        save_dir.mkdir(parents=True, exist_ok=True)\n",
      "        per_case_csv = save_dir / f\"{model_id.replace(':', '_')}_per_case.csv\"\n",
      "        if results_rows:\n",
      "            # compute union of all keys for fieldnames to include dynamic entity_f_* columns\n",
      "            all_keys = set()\n",
      "            for r in results_rows:\n",
      "                all_keys.update(r.keys())\n",
      "            # preserve a stable order: put common fields first if present\n",
      "            common_order = [\"model\", \"id\", \"rouge_l_clin\", \"bleu_clin\", \"rouge_l_lay\", \"bleu_lay\",\n",
      "                            \"entity_p\", \"entity_r\", \"entity_f\", \"hallucination_rate\",\n",
      "                            \"clinical_utility_likert\", \"pred_entities\"]\n",
      "            # remaining keys (like entity_f_xxx)\n",
      "            remaining = sorted(k for k in all_keys if k not in common_order)\n",
      "            fieldnames = [k for k in common_order if k in all_keys] + remaining\n",
      "\n",
      "            with open(per_case_csv, \"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
      "                writer = csv.DictWriter(fh, fieldnames=fieldnames)\n",
      "                writer.writeheader()\n",
      "                writer.writerows(results_rows)\n",
      "\n",
      "    # Optionally produce summary statistics (means) while leaving the agg lists intact\n",
      "    def _mean_or_none(lst):\n",
      "        try:\n",
      "            return statistics.mean(lst) if lst else None\n",
      "        except Exception:\n",
      "            return None\n",
      "\n",
      "    agg_summary = {\n",
      "        'model': model_id,\n",
      "        'n': agg['n'],\n",
      "        'mean_rouge_l_clin': _mean_or_none(agg['rouge_l_clin']),\n",
      "        'mean_bleu_clin': _mean_or_none(agg['bleu_clin']),\n",
      "        'mean_rouge_l_lay': _mean_or_none(agg['rouge_l_lay']),\n",
      "        'mean_bleu_lay': _mean_or_none(agg['bleu_lay']),\n",
      "        'mean_entity_p': _mean_or_none(agg['entity_p']),\n",
      "        'mean_entity_r': _mean_or_none(agg['entity_r']),\n",
      "        'mean_entity_f': _mean_or_none(agg['entity_f']),\n",
      "        'mean_hall_rate': _mean_or_none(agg['hall_rate']),\n",
      "        'mean_likert': _mean_or_none(agg['likert']),\n",
      "    }\n",
      "\n",
      "    # Return both the raw agg (with lists) and a compact summary for convenience\n",
      "    return {\"agg\": agg, \"summary\": agg_summary}\n",
      "\n",
      "    def _mean(lst):\n",
      "        return float(sum(lst) / len(lst)) if lst else 0.0\n",
      "\n",
      "        summary = {\n",
      "            \"model\": model_id,\n",
      "            \"n\": agg['n'],\n",
      "            \"rouge_l_clin_mean\": _mean(agg['rouge_l_clin']),\n",
      "            \"bleu_clin_mean\": _mean(agg['bleu_clin']),\n",
      "            \"rouge_l_lay_mean\": _mean(agg['rouge_l_lay']),\n",
      "            \"bleu_lay_mean\": _mean(agg['bleu_lay']),\n",
      "            \"entity_p_mean\": _mean(agg['entity_p']),\n",
      "            \"entity_r_mean\": _mean(agg['entity_r']),\n",
      "            \"entity_f_mean\": _mean(agg['entity_f']),\n",
      "            \"hallucination_rate_mean\": _mean(agg['hall_rate']),\n",
      "            \"clinical_utility_likert_mean\": _mean(agg['likert'])\n",
      "        }\n",
      "        if save_dir:\n",
      "            with open(save_dir / f\"{model_id.replace(':','_')}_summary.json\", \"w\", encoding=\"utf-8\") as fh:\n",
      "                json.dump(summary, fh, indent=2)\n",
      "        return summary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) reload the module so the updated function is used\n",
    "import importlib\n",
    "import src.evaluate_models as eval_models\n",
    "importlib.reload(eval_models)\n",
    "\n",
    "# 2) (optional) import the function directly for convenience\n",
    "from src.evaluate_models import evaluate_one_model_on_dataset\n",
    "\n",
    "# 3) confirm the function source (sanity check)\n",
    "import inspect\n",
    "print(inspect.getsource(evaluate_one_model_on_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7270308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data/gold/pending\")\n",
    "RESULTS_DIR = Path(\"results/eval_notebook\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "models = [\"openai:gpt-3.5-turbo\", \"groq:llama3-8b-8192\", \"sshleifer/distilbart-cnn-12-6\"]\n",
    "MAX_SAMPLES = 200\n",
    "DEVICE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f75e9207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: d:\\MSc_AI\\MSc_Project\\NeuroSummarize\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# Show current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Look for any gold folders under project\n",
    "for p in Path(\".\").rglob(\"gold\"):\n",
    "    print(\"Found gold folder at:\", p.resolve())\n",
    "    files = list(p.glob(\"*.json\"))\n",
    "    print(f\"  Contains {len(files)} JSON files\")\n",
    "    if files:\n",
    "        print(\"  Example:\", files[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "867491cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root detected as: D:\\MSc_AI\\MSc_Project\\NeuroSummarize\n",
      "Found 134 JSON files in D:\\MSc_AI\\MSc_Project\\NeuroSummarize\\data\\gold\\pending\n",
      "Example file: sub-10159_pending.json\n"
     ]
    }
   ],
   "source": [
    "cwd = Path().resolve()\n",
    "root = None\n",
    "for parent in [cwd] + list(cwd.parents):\n",
    "    if (parent / \"data\" / \"gold\").exists():\n",
    "        root = parent\n",
    "        break\n",
    "\n",
    "if root is None:\n",
    "    raise RuntimeError(\"Could not locate the project root containing data/gold\")\n",
    "\n",
    "print(\"Project root detected as:\", root)\n",
    "\n",
    "# --- Step 2: choose dataset ---\n",
    "USE_PENDING = False   # set True for pending predictions, False for curated gold\n",
    "\n",
    "gold_dir = root / \"data\" / \"gold\" / \"pending\"\n",
    "\n",
    "# --- Step 3: list JSON files ---\n",
    "json_files = list(gold_dir.glob(\"*.json\"))\n",
    "print(f\"Found {len(json_files)} JSON files in {gold_dir}\")\n",
    "\n",
    "if json_files:\n",
    "    print(\"Example file:\", json_files[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9934bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 134 prediction JSONs from D:\\MSc_AI\\MSc_Project\\NeuroSummarize\\data\\gold\\pending\n",
      "First prediction keys: ['id', 'report', 'clinical_summary_model', 'lay_summary_model', 'pred_entities']\n"
     ]
    }
   ],
   "source": [
    "def load_all_predictions(gold_dir=gold_dir):\n",
    "    \"\"\"Load all saved prediction JSONs from gold_dir into memory.\"\"\"\n",
    "    data = []\n",
    "    for f in gold_dir.glob(\"*.json\"):\n",
    "        try:\n",
    "            with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "                obj = json.load(fh)\n",
    "                data.append(obj)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {f.name}: {e}\")\n",
    "    print(f\"Loaded {len(data)} prediction JSONs from {gold_dir}\")\n",
    "    return data\n",
    "\n",
    "all_preds = load_all_predictions()\n",
    "print(\"First prediction keys:\", list(all_preds[0].keys()) if all_preds else \"No data found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7337b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove OpenAI models if quota issues\n",
    "# models = [m for m in models if not m.startswith(\"openai:\")]\n",
    "# print(\"Models for evaluation (after filtering):\", models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dabd37cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit number of samples for quicker testing\n",
    "MAX_SAMPLES = 20  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd436de",
   "metadata": {},
   "source": [
    "# Load gold dataset and sanity-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "177dd46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\") or config.get(\"models\", {}).get(\"groq\", {}).get(\"api_key\")\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"Groq API key not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a383b719",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = RESULTS_DIR / model_id.replace(\":\", \"_\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)  # you already do this\n",
    "\n",
    "# Ensure evaluate_one_model_on_dataset doesn’t add another subfolder automatically\n",
    "# OR modify evaluate function to create the directory:\n",
    "per_case_csv = out_dir / f\"{model_id.replace(':','_')}_per_case.csv\"\n",
    "per_case_csv.parent.mkdir(parents=True, exist_ok=True)  # <-- ensures folder exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "96e307c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 134 items from D:\\MSc_AI\\MSc_Project\\NeuroSummarize\\data\\gold\\pending\n",
      "Running: openai:gpt-3.5-turbo\n",
      " → Evaluating openai:gpt-3.5-turbo ... this may take a while\n",
      "Running: groq:llama3-8b-8192\n",
      " → Evaluating groq:llama3-8b-8192 ... this may take a while\n",
      "Running: facebook/bart-large-cnn\n",
      " → Evaluating facebook/bart-large-cnn ... this may take a while\n"
     ]
    }
   ],
   "source": [
    "items = load_gold_dataset(gold_dir)\n",
    "summaries = []\n",
    "per_case_csvs = []\n",
    "\n",
    "for model_id in models:\n",
    "    print(\"Running:\", model_id)\n",
    "    out_dir = RESULTS_DIR / model_id.replace(\":\", \"_\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Cached result files\n",
    "    per_case_file = out_dir / f\"{model_id.replace(':','_')}_per_case.csv\"\n",
    "    summary_file   = out_dir / \"summary.json\"\n",
    "\n",
    "    if per_case_file.exists() and summary_file.exists():\n",
    "        print(f\" → Skipping {model_id}, results already cached.\")\n",
    "        with open(summary_file, \"r\", encoding=\"utf-8\") as fh:\n",
    "            summary = json.load(fh)\n",
    "    else:\n",
    "        print(f\" → Evaluating {model_id} ... this may take a while\")\n",
    "        s = Summarizer(clinical_model=model_id, lay_model=model_id, device=DEVICE)\n",
    "        summary = evaluate_one_model_on_dataset(\n",
    "            model_id, items, s, max_samples=MAX_SAMPLES, save_dir=out_dir\n",
    "        )\n",
    "        # Save summary to JSON for future reuse\n",
    "        with open(summary_file, \"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump(summary, fh, indent=2)\n",
    "\n",
    "    summaries.append(summary)\n",
    "    if per_case_file.exists():\n",
    "        per_case_csvs.append(per_case_file)\n",
    "\n",
    "# Aggregate results\n",
    "pd.DataFrame(summaries).to_csv(RESULTS_DIR / \"aggregate_summary.csv\", index=False)\n",
    "with open(RESULTS_DIR / \"aggregate_summary.json\",\"w\") as fh:\n",
    "    json.dump(summaries, fh, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7cdcc1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "500c80b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS_DIR = D:\\MSc_AI\\MSc_Project\\NeuroSummarize\\notebooks\\results\\eval_notebook\n",
      "Expected JSON: D:\\MSc_AI\\MSc_Project\\NeuroSummarize\\notebooks\\results\\eval_notebook\\aggregate_summary.json\n"
     ]
    }
   ],
   "source": [
    "print(\"RESULTS_DIR =\", RESULTS_DIR.resolve())\n",
    "print(\"Expected JSON:\", (RESULTS_DIR / \"aggregate_summary.json\").resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c4a8eb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 134 prediction JSONs from D:\\MSc_AI\\MSc_Project\\NeuroSummarize\\data\\gold\\pending\n",
      "Loaded 134 cases\n"
     ]
    }
   ],
   "source": [
    "all_preds = load_all_predictions()  # from gold or pending\n",
    "print(\"Loaded\", len(all_preds), \"cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f8f95a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>n_cases</th>\n",
       "      <th>rouge_clinical</th>\n",
       "      <th>rouge_lay</th>\n",
       "      <th>bleu_clinical</th>\n",
       "      <th>bleu_lay</th>\n",
       "      <th>entity_f1</th>\n",
       "      <th>hallucination_rate</th>\n",
       "      <th>utility_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>openai:gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>groq:llama3-8b-8192</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facebook/bart-large-cnn</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model  n_cases rouge_clinical rouge_lay bleu_clinical  \\\n",
       "0     openai:gpt-3.5-turbo        0           None      None          None   \n",
       "1      groq:llama3-8b-8192        0           None      None          None   \n",
       "2  facebook/bart-large-cnn        0           None      None          None   \n",
       "\n",
       "  bleu_lay entity_f1 hallucination_rate utility_mean  \n",
       "0     None      None               None         None  \n",
       "1     None      None               None         None  \n",
       "2     None      None               None         None  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for model_id in models:\n",
    "    summary = evaluate_one_model_on_dataset(model_id, items, s, max_samples=MAX_SAMPLES)\n",
    "    agg = summary['summary']  # This has mean metrics already\n",
    "    results.append({\n",
    "        \"model\": model_id,\n",
    "        \"n_cases\": agg['n'],\n",
    "        \"rouge_clinical\": agg['mean_rouge_l_clin'],\n",
    "        \"rouge_lay\": agg['mean_rouge_l_lay'],\n",
    "        \"bleu_clinical\": agg['mean_bleu_clin'],\n",
    "        \"bleu_lay\": agg['mean_bleu_lay'],\n",
    "        \"entity_f1\": agg['mean_entity_f'],\n",
    "        \"hallucination_rate\": agg['mean_hall_rate'],\n",
    "        \"utility_mean\": agg['mean_likert']\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
